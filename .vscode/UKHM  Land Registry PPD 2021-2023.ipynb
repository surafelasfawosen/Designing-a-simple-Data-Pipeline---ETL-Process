{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "826dc84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined DataFrame shape: (4535701, 16)\n",
      "Combined DataFrame columns: ['transaction_id', 'price', 'date_of_transfer', 'postcode', 'property_type', 'old_new', 'duration', 'paon', 'saon', 'street', 'locality', 'town_city', 'district', 'county', 'ppd_category_type', 'record_status']\n",
      "Sample of combined data:\n",
      "                           transaction_id   price  date_of_transfer  postcode  \\\n",
      "0  {CB0035E6-3546-58AE-E053-6B04A8C091AF}  630000  2021-04-29 00:00  BN13 3AH   \n",
      "1  {CB0035E6-3547-58AE-E053-6B04A8C091AF}  477000  2021-05-10 00:00  BN16 2PQ   \n",
      "2  {CB0035E6-3548-58AE-E053-6B04A8C091AF}  370000  2021-05-06 00:00   BN2 4HZ   \n",
      "3  {CB0035E6-3549-58AE-E053-6B04A8C091AF}  462500  2021-04-28 00:00   BN2 0GP   \n",
      "4  {CB0035E6-354A-58AE-E053-6B04A8C091AF}  433000  2021-08-06 00:00   BN8 4LS   \n",
      "\n",
      "  property_type old_new duration paon saon           street    locality  \\\n",
      "0             D       N        F    3  NaN  HIGHLANDS CLOSE         NaN   \n",
      "1             D       N        F   10  NaN   CHAUCER AVENUE  RUSTINGTON   \n",
      "2             T       N        F    6  NaN  DARTMOUTH CLOSE         NaN   \n",
      "3             T       N        F   18  NaN   STANLEY STREET         NaN   \n",
      "4             D       N        F   11  NaN      POWELL ROAD      NEWICK   \n",
      "\n",
      "       town_city           district             county ppd_category_type  \\\n",
      "0       WORTHING           WORTHING        WEST SUSSEX                 A   \n",
      "1  LITTLEHAMPTON               ARUN        WEST SUSSEX                 A   \n",
      "2       BRIGHTON  BRIGHTON AND HOVE  BRIGHTON AND HOVE                 A   \n",
      "3       BRIGHTON  BRIGHTON AND HOVE  BRIGHTON AND HOVE                 A   \n",
      "4          LEWES              LEWES        EAST SUSSEX                 A   \n",
      "\n",
      "  record_status  \n",
      "0             A  \n",
      "1             A  \n",
      "2             A  \n",
      "3             A  \n",
      "4             A  \n",
      "\n",
      "Saved combined data to C:\\Users\\SURA\\OneDrive\\Desktop\\UKHM  Land Registry PPD 2021-2023\\combined_ukhm.csv\n",
      "Extraction and combination complete!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the base path\n",
    "base_path = r\"C:\\Users\\SURA\\OneDrive\\Desktop\\UKHM  Land Registry PPD 2021-2023\"\n",
    "\n",
    "# List of CSV files\n",
    "csv_files = [\n",
    "    os.path.join(base_path, \"pp-2021.csv\"),\n",
    "    os.path.join(base_path, \"pp-2022.csv\"),\n",
    "    os.path.join(base_path, \"pp-2025.csv\"),\n",
    "    os.path.join(base_path, \"pp-2024.csv\"),\n",
    "    os.path.join(base_path, \"pp-2023.csv\")\n",
    "]\n",
    "\n",
    "# Define column names for HM Land Registry Price Paid Data\n",
    "columns = [\n",
    "    \"transaction_id\",         # Unique transaction GUID\n",
    "    \"price\",                  # Sale price (¬£)\n",
    "    \"date_of_transfer\",       # Date of sale\n",
    "    \"postcode\",               # Postcode\n",
    "    \"property_type\",          # D=Detached, S=Semi, T=Terraced, F=Flat, O=Other\n",
    "    \"old_new\",                # Y=New build, N=Old\n",
    "    \"duration\",               # F=Freehold, L=Leasehold\n",
    "    \"paon\",                   # Primary addressable object\n",
    "    \"saon\",                   # Secondary addressable object\n",
    "    \"street\",                 # Street name\n",
    "    \"locality\",               # Optional sub-locality\n",
    "    \"town_city\",              # Town or city\n",
    "    \"district\",               # Local authority district\n",
    "    \"county\",                 # County\n",
    "    \"ppd_category_type\",      # A=Standard, B=Additional\n",
    "    \"record_status\"           # A=Added, C=Changed\n",
    "]\n",
    "\n",
    "# Load and combine all CSV files into one DataFrame with defined columns\n",
    "dataframes = [pd.read_csv(file, header=None, names=columns, encoding='utf-8', low_memory=False) for file in csv_files]\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Print details to verify\n",
    "print(f\"Combined DataFrame shape: {combined_df.shape}\")\n",
    "print(f\"Combined DataFrame columns: {combined_df.columns.tolist()}\")\n",
    "print(f\"Sample of combined data:\\n{combined_df.head()}\\n\")\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file\n",
    "output_file = os.path.join(base_path, \"combined_ukhm.csv\")\n",
    "combined_df.to_csv(output_file, index=False)\n",
    "print(f\"Saved combined data to {output_file}\")\n",
    "\n",
    "print(\"Extraction and combination complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62120f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initial Data Overview ---\n",
      "Original shape: (4535701, 16)\n",
      "\n",
      "--- Missing Values Before Column Drops ---\n",
      "          Missing Count  Missing Percent\n",
      "saon            3939136        86.847347\n",
      "locality        2807270        61.892748\n",
      "street            77967         1.718963\n",
      "postcode          12179         0.268514\n",
      "\n",
      "--- Dropping Columns with >= 60% Missing Values ---\n",
      "Columns to drop: ['saon', 'locality']\n",
      "Shape after dropping high-missing columns: (4535701, 14)\n",
      "\n",
      "--- Duplicate Rows Before Cleaning ---\n",
      "No duplicate rows found initially.\n",
      "\n",
      "--- Performing Data Transformations ---\n",
      "\n",
      "Shape before dropping rows with missing 'price' or 'date_of_transfer': (4535701, 14)\n",
      "Dropped 0 rows due to missing 'price' or 'date_of_transfer'.\n",
      "Shape after dropping rows with missing 'price' or 'date_of_transfer': (4535701, 14)\n",
      "\n",
      "--- Handling Duplicate Rows (Post-Transformation) ---\n",
      "No duplicate rows to remove after transformations and critical NA drop.\n",
      "\n",
      "--- Final Data Overview ---\n",
      "‚úÖ CSV Loaded and Transformed Successfully!\n",
      "Shape: (4535701, 14)\n",
      "Column names: ['transaction_id', 'price', 'date_of_transfer', 'postcode', 'property_type', 'old_new', 'duration', 'paon', 'street', 'town_city', 'district', 'county', 'ppd_category_type', 'record_status']\n",
      "\n",
      "üè† Price Summary:\n",
      "count    4.535701e+06\n",
      "mean     3.964401e+05\n",
      "std      1.553015e+06\n",
      "min      1.000000e+00\n",
      "25%      1.780000e+05\n",
      "50%      2.750000e+05\n",
      "75%      4.249500e+05\n",
      "max      9.000000e+08\n",
      "Name: price, dtype: float64\n",
      "\n",
      "Property Type Distribution:\n",
      "property_type\n",
      "Terraced           1237811\n",
      "Semi-Detached      1208256\n",
      "Detached           1043606\n",
      "Flat/Maisonette     810018\n",
      "Other               236010\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Missing Values After ALL Cleaning ---\n",
      "          Missing Count  Missing Percent\n",
      "street            77967         1.718963\n",
      "postcode          12179         0.268514\n",
      "\n",
      "Transformation complete! Cleaned data saved to C:\\Users\\SURA\\OneDrive\\Desktop\\UKHM  Land Registry PPD 2021-2023\\combined_ukhm_clean.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Input and output file paths\n",
    "input_file = r\"C:\\Users\\SURA\\OneDrive\\Desktop\\UKHM  Land Registry PPD 2021-2023\\combined_ukhm.csv\"\n",
    "output_file = r\"C:\\Users\\SURA\\OneDrive\\Desktop\\UKHM  Land Registry PPD 2021-2023\\combined_ukhm_clean.csv\"\n",
    "\n",
    "# Define expected column names\n",
    "expected_columns = [\n",
    "    \"transaction_id\",         # Unique transaction GUID\n",
    "    \"price\",                  # Sale price (¬£)\n",
    "    \"date_of_transfer\",       # Date of sale\n",
    "    \"postcode\",               # Postcode\n",
    "    \"property_type\",          # D=Detached, S=Semi, T=Terraced, F=Flat, O=Other (to be transformed)\n",
    "    \"old_new\",                # Y=New build, N=Old\n",
    "    \"duration\",               # F=Freehold, L=Leasehold\n",
    "    \"paon\",                   # Primary addressable object\n",
    "    \"saon\",                   # Secondary addressable object\n",
    "    \"street\",                 # Street name\n",
    "    \"locality\",               # Optional sub-locality\n",
    "    \"town_city\",              # Town or city\n",
    "    \"district\",               # Local authority district\n",
    "    \"county\",                 # County\n",
    "    \"ppd_category_type\",      # A=Standard, B=Additional\n",
    "    \"record_status\"           # A=Added, C=Changed\n",
    "]\n",
    "\n",
    "# Mapping dictionaries for all encoded columns\n",
    "property_type_map = {\n",
    "    \"D\": \"Detached\",\n",
    "    \"S\": \"Semi-Detached\",\n",
    "    \"T\": \"Terraced\",\n",
    "    \"F\": \"Flat/Maisonette\",\n",
    "    \"O\": \"Other\"\n",
    "}\n",
    "\n",
    "old_new_map = {\n",
    "    \"Y\": \"New Build\",\n",
    "    \"N\": \"Established\"\n",
    "}\n",
    "\n",
    "duration_map = {\n",
    "    \"F\": \"Freehold\",\n",
    "    \"L\": \"Leasehold\"\n",
    "}\n",
    "\n",
    "ppd_category_map = {\n",
    "    \"A\": \"Standard Price Paid Entry\",\n",
    "    \"B\": \"Additional Price Paid Entry\"\n",
    "}\n",
    "\n",
    "record_status_map = {\n",
    "    \"A\": \"Added\",\n",
    "    \"C\": \"Changed\",\n",
    "    \"D\": \"Deleted\"\n",
    "}\n",
    "\n",
    "# Load the combined CSV file\n",
    "df = pd.read_csv(input_file, encoding='utf-8', low_memory=False)\n",
    "\n",
    "print(\"--- Initial Data Overview ---\")\n",
    "print(f\"Original shape: {df.shape}\")\n",
    "\n",
    "# 1. Identify and report missing values BEFORE any major drops/transformations\n",
    "print(\"\\n--- Missing Values Before Column Drops ---\")\n",
    "missing_values_count_initial = df.isnull().sum()\n",
    "missing_values_percent_initial = 100 * df.isnull().sum() / len(df)\n",
    "initial_missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_values_count_initial,\n",
    "    'Missing Percent': missing_values_percent_initial\n",
    "})\n",
    "initial_missing_df = initial_missing_df[initial_missing_df['Missing Count'] > 0].sort_values(by='Missing Count', ascending=False)\n",
    "if not initial_missing_df.empty:\n",
    "    print(initial_missing_df)\n",
    "else:\n",
    "    print(\"No missing values found initially.\")\n",
    "\n",
    "# 2. Identify columns to drop based on a threshold (e.g., 60%)\n",
    "missing_threshold = 60\n",
    "cols_to_drop = initial_missing_df[initial_missing_df['Missing Percent'] >= missing_threshold].index.tolist()\n",
    "\n",
    "if cols_to_drop:\n",
    "    print(f\"\\n--- Dropping Columns with >= {missing_threshold}% Missing Values ---\")\n",
    "    print(f\"Columns to drop: {cols_to_drop}\")\n",
    "    df.drop(columns=cols_to_drop, inplace=True)\n",
    "    print(f\"Shape after dropping high-missing columns: {df.shape}\")\n",
    "else:\n",
    "    print(f\"\\nNo columns found with >= {missing_threshold}% missing values.\")\n",
    "\n",
    "# 3. Identify and report duplicate rows\n",
    "print(\"\\n--- Duplicate Rows Before Cleaning ---\")\n",
    "initial_duplicates = df.duplicated().sum()\n",
    "if initial_duplicates > 0:\n",
    "    print(f\"Found {initial_duplicates} duplicate rows.\")\n",
    "else:\n",
    "    print(\"No duplicate rows found initially.\")\n",
    "\n",
    "# --- Perform Transformations ---\n",
    "print(\"\\n--- Performing Data Transformations ---\")\n",
    "df[\"price\"] = pd.to_numeric(df[\"price\"], errors=\"coerce\")          # Convert price to numeric\n",
    "df[\"date_of_transfer\"] = pd.to_datetime(df[\"date_of_transfer\"], errors=\"coerce\")  # Convert date\n",
    "\n",
    "# ‚úÖ Apply mapping for categorical codes to readable labels\n",
    "if \"property_type\" in df.columns:\n",
    "    df[\"property_type\"] = df[\"property_type\"].map(property_type_map).fillna(df[\"property_type\"])\n",
    "\n",
    "if \"old_new\" in df.columns:\n",
    "    df[\"old_new\"] = df[\"old_new\"].map(old_new_map).fillna(df[\"old_new\"])\n",
    "\n",
    "if \"duration\" in df.columns:\n",
    "    df[\"duration\"] = df[\"duration\"].map(duration_map).fillna(df[\"duration\"])\n",
    "\n",
    "if \"ppd_category_type\" in df.columns:\n",
    "    df[\"ppd_category_type\"] = df[\"ppd_category_type\"].map(ppd_category_map).fillna(df[\"ppd_category_type\"])\n",
    "\n",
    "if \"record_status\" in df.columns:\n",
    "    df[\"record_status\"] = df[\"record_status\"].map(record_status_map).fillna(df[\"record_status\"])\n",
    "\n",
    "# 4. Drop rows with missing 'price' or 'date_of_transfer' AFTER conversion\n",
    "print(f\"\\nShape before dropping rows with missing 'price' or 'date_of_transfer': {df.shape}\")\n",
    "rows_before_dropping_critical_na = df.shape[0]\n",
    "df = df.dropna(subset=[\"price\", \"date_of_transfer\"])\n",
    "rows_after_dropping_critical_na = df.shape[0]\n",
    "print(f\"Dropped {rows_before_dropping_critical_na - rows_after_dropping_critical_na} rows due to missing 'price' or 'date_of_transfer'.\")\n",
    "print(f\"Shape after dropping rows with missing 'price' or 'date_of_transfer': {df.shape}\")\n",
    "\n",
    "# 5. Handle duplicates after cleaning\n",
    "print(\"\\n--- Handling Duplicate Rows (Post-Transformation) ---\")\n",
    "original_rows_post_transform = df.shape[0]\n",
    "df.drop_duplicates(inplace=True)\n",
    "rows_after_dropping_duplicates = df.shape[0]\n",
    "duplicates_removed = original_rows_post_transform - rows_after_dropping_duplicates\n",
    "if duplicates_removed > 0:\n",
    "    print(f\"Removed {duplicates_removed} duplicate rows.\")\n",
    "else:\n",
    "    print(\"No duplicate rows to remove after transformations and critical NA drop.\")\n",
    "\n",
    "# --- Final Reporting ---\n",
    "print(\"\\n--- Final Data Overview ---\")\n",
    "print(\"‚úÖ CSV Loaded and Transformed Successfully!\")\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Column names:\", df.columns.tolist())\n",
    "\n",
    "print(\"\\nüè† Price Summary:\")\n",
    "print(df[\"price\"].describe())\n",
    "\n",
    "print(\"\\nProperty Type Distribution:\")\n",
    "print(df[\"property_type\"].value_counts())\n",
    "\n",
    "# Report missing values after ALL cleaning\n",
    "print(\"\\n--- Missing Values After ALL Cleaning ---\")\n",
    "missing_values_count_final = df.isnull().sum()\n",
    "missing_values_percent_final = 100 * df.isnull().sum() / len(df)\n",
    "final_missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_values_count_final,\n",
    "    'Missing Percent': missing_values_percent_final\n",
    "})\n",
    "final_missing_df = final_missing_df[final_missing_df['Missing Count'] > 0].sort_values(by='Missing Count', ascending=False)\n",
    "if not final_missing_df.empty:\n",
    "    print(final_missing_df)\n",
    "else:\n",
    "    print(\"No missing values remaining after all cleaning steps.\")\n",
    "\n",
    "# Save the clean DataFrame\n",
    "df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "print(f\"\\nTransformation complete! Cleaned data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce7c38a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e8363e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving clean data to 'combined_ukhm_clean.parquet'...\n",
      "Clean data saved successfully. You can now run the load_to_db.py script.\n"
     ]
    }
   ],
   "source": [
    "# This file will be the input for our load_to_db.py script.\n",
    "\n",
    "output_file = 'combined_ukhm_clean.parquet' # <-- Correct the filename and extension\n",
    "print(f\"Saving clean data to '{output_file}'...\")\n",
    "\n",
    "df.to_parquet(output_file, index=False) # This will now save a proper .parquet file\n",
    "\n",
    "print(\"Clean data saved successfully. You can now run the load_to_db.py script.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef3490c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
